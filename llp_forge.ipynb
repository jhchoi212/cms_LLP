{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08b50da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ntuple0;1', 'ntuple0/objects;1']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "import ROOT, sys\n",
    "import uproot as ur\n",
    "from ROOT import *\n",
    "import numpy\n",
    "import h5py\n",
    "\n",
    "pt_cut = 30\n",
    "train_test_ratio = 80\n",
    "PF_PUPPI = 1\n",
    "generated_file = \"LLP_reco\"\n",
    "\n",
    "ROOT.gROOT.SetBatch(1)\n",
    "\n",
    "inFileName = 'pfTuple_1.root'\n",
    "inFile = ROOT.TFile.Open(inFileName)\n",
    "\n",
    "\n",
    "tree = inFile.Get(\"ntuple0/objects\")\n",
    "ver = inFile.Get(\"ntuple0/objects/vz\")\n",
    "\n",
    "fileName = 'pfTuple_1.root'\n",
    "\n",
    "file = ur.open(fileName)\n",
    "file.classnames()\n",
    "print(file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6988027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f7e0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load variables based on inputs and initialize lists to be used later\n",
    "eventNum = tree.GetEntries()\n",
    "pTCut = float(pt_cut)\n",
    "jetList = []\n",
    "jetNum = 0\n",
    "eventJets = []\n",
    "LLPCount = 0\n",
    "jetPartList = []\n",
    "trainArray = []\n",
    "testArray = []\n",
    "jetFullData = []\n",
    "trainingFullData = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcaf57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalePartType(a, n):\n",
    "    if n==11:\n",
    "        a.extend((1,0,0,0,0,0,0,0,0,0)) #Electron\n",
    "    elif n==-11:\n",
    "        a.extend((0,1,0,0,0,0,0,0,0,0)) #Positron\n",
    "    elif n==13:\n",
    "        a.extend((0,0,1,0,0,0,0,0,0,0)) #Muon\n",
    "    elif n==-13:\n",
    "        a.extend((0,0,0,1,0,0,0,0,0,0)) #Anti-Muon\n",
    "    elif n==22:\n",
    "        a.extend((0,0,0,0,1,0,0,0,0,0)) #Photon\n",
    "    elif n==130:\n",
    "        a.extend((0,0,0,0,0,1,0,0,0,0)) #Neutral Meson\n",
    "    elif n==211:\n",
    "        a.extend((0,0,0,0,0,0,1,0,0,0)) #Pion\n",
    "    elif n==-211:\n",
    "        a.extend((0,0,0,0,0,0,0,1,0,0)) #Anti-Pion\n",
    "    elif n== 1000006:\n",
    "        a.extend((0,0,0,0,0,0,0,0,1,0))\n",
    "    elif n== -1000006:\n",
    "        a.extend((0,0,0,0,0,0,0,0,0,1))\n",
    "    else:\n",
    "        a.extend((0,0,0,0,0,0,0,0,0,0)) #Case for unknown particle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd541bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling Phi for particles relative to their jet\n",
    "def signedDeltaPhi(phi1, phi2):\n",
    "    dPhi = phi1 - phi2\n",
    "    if (dPhi < -numpy.pi):\n",
    "        dPhi = 2 * numpy.pi + dPhi\n",
    "    elif (dPhi > numpy.pi):\n",
    "        dPhi = -2 * numpy.pi + dPhi\n",
    "    return dPhi\n",
    "\n",
    "jetPartsArray = []\n",
    "jetDataArray = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cabc6cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Jet Construction\n",
      "Progress: 0 out of 168, approximately 0% complete.\n",
      "Current No. of Jets: 0\n",
      "Current No. of Signal Jets: 0\n",
      "Progress: 16 out of 168, approximately 9% complete.\n",
      "Current No. of Jets: 61\n",
      "Current No. of Signal Jets: 8\n",
      "Progress: 32 out of 168, approximately 19% complete.\n",
      "Current No. of Jets: 125\n",
      "Current No. of Signal Jets: 15\n",
      "Progress: 48 out of 168, approximately 28% complete.\n",
      "Current No. of Jets: 192\n",
      "Current No. of Signal Jets: 22\n",
      "Progress: 64 out of 168, approximately 38% complete.\n",
      "Current No. of Jets: 252\n",
      "Current No. of Signal Jets: 28\n",
      "Progress: 80 out of 168, approximately 47% complete.\n",
      "Current No. of Jets: 318\n",
      "Current No. of Signal Jets: 36\n",
      "Progress: 96 out of 168, approximately 57% complete.\n",
      "Current No. of Jets: 386\n",
      "Current No. of Signal Jets: 45\n",
      "Progress: 112 out of 168, approximately 66% complete.\n",
      "Current No. of Jets: 456\n",
      "Current No. of Signal Jets: 51\n",
      "Progress: 128 out of 168, approximately 76% complete.\n",
      "Current No. of Jets: 519\n",
      "Current No. of Signal Jets: 58\n",
      "Progress: 144 out of 168, approximately 85% complete.\n",
      "Current No. of Jets: 589\n",
      "Current No. of Signal Jets: 62\n",
      "Progress: 160 out of 168, approximately 95% complete.\n",
      "Current No. of Jets: 655\n",
      "Current No. of Signal Jets: 71\n"
     ]
    }
   ],
   "source": [
    "print('Beginning Jet Construction')\n",
    "for entryNum in range(eventNum):\n",
    "    if entryNum%(int(eventNum/10))==0:\n",
    "        print('Progress: '+str(entryNum)+\" out of \" + str(eventNum)+\", approximately \"+str(int(100*entryNum/eventNum))+\"%\"+\" complete.\")\n",
    "        print('Current No. of Jets: '+str(len(jetPartsArray)))\n",
    "        print('Current No. of Signal Jets: '+str(LLPCount))\n",
    "    tree.GetEntry(entryNum)\n",
    "    ver = tree.vz\n",
    "    #Loading particle candidates based on PF or PUPPI input\n",
    "##    if int(PF_PUPPI)==0:\n",
    "##        obj = tree.pf\n",
    "##        verPf = tree.pf_vz\n",
    "##        verPfX = tree.pf_vx\n",
    "##        verPfY = tree.pf_vy\n",
    "    if int(PF_PUPPI)==1:\n",
    "        obj = tree.pup\n",
    "        verPf = tree.pup_vz\n",
    "        verPfX = tree.pup_vx\n",
    "        verPfY = tree.pup_vy\n",
    "    jetNum = 0\n",
    "    bannedParts = [] #List of indices of particles that have already been used by previous jets\n",
    "    bannedLLPs = [] #Same deal but with indices within the gen tree corresponding to b quarks\n",
    "\n",
    "    #Loops through pf/pup candidates\n",
    "    for i in range(len(obj)):\n",
    "        jetPartList = []\n",
    "        seedParticle = []\n",
    "        if jetNum>=12: #Limited to 12 jets per event at maximum\n",
    "            jetNum = 0\n",
    "            break\n",
    "        if i not in bannedParts: #Identifies highest avaiable pT particle to use as seed\n",
    "            tempTLV = obj[i][0] #Takes TLorentzVector of seed particle to use for jet reconstruction\n",
    "            scalePartType(seedParticle,abs(obj[i][1])) #One-Hot Encoding Seed Particle Type\n",
    "            if obj[i][1]==22 or obj[i][1]==130:\n",
    "                seedParticle.extend([0.0,verPfX[i],verPfY[i],obj[i][0].Pt(),obj[i][0].Eta(),obj[i][0].Phi()]) #Add in dZ, dX, dY, Particle Pt, Eta, & Phi, last 3 features to be scaled later\n",
    "            else:\n",
    "                seedParticle.extend([ver[0]-verPf[i],verPfX[i],verPfY[i],obj[i][0].Pt(),obj[i][0].Eta(),obj[i][0].Phi()]) #Add in dZ, dX, dY, Particle Pt, Eta, & Phi, last 3 features to be scaled later\n",
    "            jetPartList.extend(seedParticle) #Add particle features to particle list\n",
    "            bannedParts.append(i) #Mark this particle as unavailable for other jets\n",
    "            for j in range(len(obj)):\n",
    "                partFts = []\n",
    "                if obj[i][0].DeltaR(obj[j][0])<=0.4 and i!=j and (j not in bannedParts): #Look for available particles within deltaR<0.4 of seed particle\n",
    "                    tempTLV=tempTLV+obj[j][0] #Add to tempTLV\n",
    "                    scalePartType(partFts,obj[j][1]) #One-Hot Encoding Particle Type\n",
    "                    if obj[j][1]==22 or obj[j][1]==130:\n",
    "                        partFts.extend([0.0,verPfX[j],verPfY[j],obj[j][0].Pt(),obj[j][0].Eta(),obj[j][0].Phi()]) #Add in dZ, dX, dY, Particle Pt, Eta, & Phi, last 3 features to be scaled later\n",
    "                    else:\n",
    "                        partFts.extend([ver[0]-verPf[j],verPfX[j],verPfY[j],obj[j][0].Pt(),obj[j][0].Eta(),obj[j][0].Phi()])\n",
    "                    jetPartList.extend(partFts)  #Add particle features to particle list\n",
    "                    bannedParts.append(j) #Mark this particle as unavailable for other jets\n",
    "                if len(jetPartList)>=10*14: #If you reach 10 particles in one jet, break and move on\n",
    "                    break\n",
    "            if abs(tempTLV.Pt())<pTCut: #Neglect to save jet if it falls below pT Cut\n",
    "                break\n",
    "            #Scaling particle pT, Eta, and Phi based on jet pT, Eta, and Phi\n",
    "            c = 11\n",
    "            while c<len(jetPartList)-2:\n",
    "                jetPartList[c]=jetPartList[c]/tempTLV.Pt()\n",
    "                jetPartList[c+1]=tempTLV.Eta()-jetPartList[c+1]\n",
    "                tempPhi = jetPartList[c+2]\n",
    "                jetPartList[c+2] = signedDeltaPhi(tempTLV.Phi(),tempPhi)\n",
    "                c+=14\n",
    "            #Ensure all inputs are same length\n",
    "            while len(jetPartList)<10*14:\n",
    "                jetPartList.append(0)\n",
    "            #Add in final value to indicate if particle is matched (1) or unmatched (0) to a gen b quark by looking for gen b quarks within deltaR<0.4 of jet\n",
    "            jetPartList.append(0)\n",
    "            for e in range(len(tree.gen)):\n",
    "                ## For abs(tree.gen[e][1])==5, find correct PDG id for reco\n",
    "                if abs(tree.gen[e][1])==1000006 and (e not in bannedLLPs) and abs(tree.gen[e][0].Eta())<2.3:\n",
    "                    if tree.gen[e][0].DeltaR(tempTLV)<=0.4:\n",
    "                        jetPartList[-1]=1\n",
    "                        LLPCount+=1\n",
    "                        bannedLLPs.append(e)\n",
    "                        break\n",
    "            #Store particle inputs and jet features in overall list\n",
    "            jetPartsArray.append(jetPartList)\n",
    "            jetDataArray.append((tempTLV.Pt(),tempTLV.Eta(),tempTLV.Phi(),tempTLV.M()))\n",
    "            jetNum+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa756f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Jets 692\n",
      "Total No. of Matched Jets: 73\n",
      "No. of Jets in Training Data: 553\n",
      "No. of Jets in Testing Data: 139\n",
      "Debug that everything matches up in length:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Break dataset into training/testing data based on train/test split input\n",
    "trainTestSplit = int(train_test_ratio)\n",
    "splitIndex = int(float(trainTestSplit)/100*len(jetPartsArray))\n",
    "trainArray = jetPartsArray[:splitIndex]\n",
    "trainingFullData = jetDataArray[:splitIndex]\n",
    "\n",
    "testArray = jetPartsArray[splitIndex:]\n",
    "jetFullData = jetDataArray[splitIndex:]\n",
    "\n",
    "print('Total Jets '+str(len(jetPartsArray)))\n",
    "print('Total No. of Matched Jets: '+str(LLPCount))\n",
    "print('No. of Jets in Training Data: '+str(len(trainArray)))\n",
    "print('No. of Jets in Testing Data: '+str(len(testArray)))\n",
    "\n",
    "#Final Check\n",
    "print('Debug that everything matches up in length:')\n",
    "print(len(testArray)==len(jetFullData) and len(trainArray)==len(trainingFullData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35a51ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/h5py/_hl/base.py:118: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.asarray(data, order=\"C\", dtype=as_dtype)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object dtype dtype('O') has no native HDF5 equivalent",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Save datasets as h5 files\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Testing Data: Particle Inputs for each jet of Shape [...,141]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtestingData\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(generated_file)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hf:\n\u001b[0;32m----> 5\u001b[0m \t\u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTesting Data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestArray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#Jet Data: Jet Features (pT, Eta, Phi, Mass) of each testing data jet of shape [...,4]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjetData\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(generated_file)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hf:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/h5py/_hl/group.py:149\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    147\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 149\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/h5py/_hl/dataset.py:91\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, allow_unknown_filter)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[0;32m---> 91\u001b[0m     tid \u001b[38;5;241m=\u001b[39m \u001b[43mh5t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Legacy\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m((compression, shuffle, fletcher32, maxshape, scaleoffset)) \u001b[38;5;129;01mand\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/h5t.pyx:1663\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1687\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1747\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object dtype dtype('O') has no native HDF5 equivalent"
     ]
    }
   ],
   "source": [
    "#Save datasets as h5 files\n",
    "\n",
    "#Testing Data: Particle Inputs for each jet of Shape [...,141]\n",
    "with h5py.File('testingData'+str(generated_file)+'.h5','w') as hf:\n",
    "\thf.create_dataset(\"Testing Data\", data=testArray)\n",
    "#Jet Data: Jet Features (pT, Eta, Phi, Mass) of each testing data jet of shape [...,4]\n",
    "with h5py.File('jetData'+str(generated_file)+'.h5','w') as hf:\n",
    "\thf.create_dataset(\"Jet Data\", data=jetFullData)\n",
    "#Training Data: Particle Inputs for each jet of Shape [...,141]\n",
    "with h5py.File('trainingData'+str(generated_file)+'.h5','w') as hf:\n",
    "\thf.create_dataset(\"Training Data\", data=trainArray)\n",
    "#Sample Data: Jet Features (pT, Eta, Phi, Mass) of each training data jet of shape [...,4]\n",
    "with h5py.File('sampleData'+str(generated_file)+'.h5','w') as hf:\n",
    "\thf.create_dataset(\"Sample Data\", data=trainingFullData)\n",
    "\n",
    "end = time.time()\n",
    "print(str(end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
